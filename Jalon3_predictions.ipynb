{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U textblob\n",
        "!python -m textblob.download_corpora\n",
        "!pip install ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-pHTGCbyzJ1",
        "outputId": "2d73da28-1214-4023-a31e-060d116013fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipynb in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqDf4N5vyhWl",
        "outputId": "15fe16d9-293e-48d0-ade0-87e59373f5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWUsrdvwq2XI",
        "outputId": "23b17f0c-a101-477f-faaf-2b7052610550"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(n_components=15)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "source": [
        "file_name=\"/content/drive/MyDrive/NLP I3/model_file.pickle\"\n",
        "with (open(file_name, \"rb\")) as f:\n",
        "    while True:\n",
        "        try:\n",
        "            model=pickle.load(f)\n",
        "        except EOFError:\n",
        "            break\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file_name=\"/content/drive/MyDrive/NLP I3/vectoriseur_file.pickle\"\n",
        "with (open(file_name, \"rb\")) as f:\n",
        "    while True:\n",
        "        try:\n",
        "            vectorizer=pickle.load(f)\n",
        "        except EOFError:\n",
        "            break\n",
        "vectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxqXK0fer7wH",
        "outputId": "c9d6dca8-46d3-4aa9-eb55-7802bb22a34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(max_df=0.8, min_df=0.02)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"Analytics Vidhya is a great platform to learn data science. \\n It helps community through blogs, hackathons, discussions,etc.\")"
      ],
      "metadata": {
        "id": "KRXkZ49ptmOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob.sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdEIlQCK0UWG",
        "outputId": "7eebc85a-4742-4256-becb-d80adbdb369b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.8, subjectivity=0.75)"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed"
      ],
      "metadata": {
        "id": "yGXmXkhl1YqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    \n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    \n",
        "    return \" \".join(lemmatized_text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWQwQKh61jYw",
        "outputId": "3f2688cf-7410-44c8-ec64-5c8e97f79044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['parser', 'tagger', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])"
      ],
      "metadata": {
        "id": "kYSGsLuK1nJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "def contraction_text(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v61DmE2G1znB",
        "outputId": "16fe8a17-6dd8-4162-fb3c-1f816ea1159f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\""
      ],
      "metadata": {
        "id": "ELBQpJ2h2Csw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "vGQssa142EMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])\n"
      ],
      "metadata": {
        "id": "I0ayyTa92GK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \n",
        "    # Tokenize review\n",
        "    text = tokenize_text(text)\n",
        "    \n",
        "    # Lemmatize review\n",
        "    text = lemmatize_text(text)\n",
        "    \n",
        "    # Normalize review\n",
        "    text = normalize_text(text)\n",
        "    \n",
        "    # Remove contractions\n",
        "    text = contraction_text(text)\n",
        "\n",
        "    # Get negative tokens\n",
        "    text = get_negative_token(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    text = remove_stopwords(text)\n",
        "    \n",
        "    return text\n",
        "    "
      ],
      "metadata": {
        "id": "JjZFnxmq1M-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text(\"Analytics Vidhya is a great platform to learn data science. \\n It helps community through blogs, hackathons, discussions,etc.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PAw_5cyE2RIC",
        "outputId": "7d9f20a5-e4bf-461b-bcb7-47eec98bfc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'analytics vidhya great platform learn data science help community blog hackathons discussion etc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=\"Very bad server and staff\""
      ],
      "metadata": {
        "id": "EG_FkzB75zd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "    #\n",
        "    0 : \"Mauvaise assistance téléphone\" ,\n",
        "    #\n",
        "    1 : \"Nourriture pas bonne\" ,\n",
        "    #\n",
        "    2 : \"Pizza\" ,\n",
        "    #\n",
        "    3 : \"Chicken\" ,\n",
        "    #\n",
        "    4 : \"Mauvaise qualité nourriture\" ,\n",
        "    #\n",
        "    5 : \"Service déplorable\" ,\n",
        "    #\n",
        "    6 : \"Burgers\" ,\n",
        "    #\n",
        "    7 : \"Trop d'attente\" ,\n",
        "    #\n",
        "    8 : \"Mauvaise expérience générale\" ,\n",
        "    #\n",
        "    9 : \"Nourriture pas bonne\" ,\n",
        "    #\n",
        "    10 : \"Mauvaise Livraison\" ,\n",
        "    #\n",
        "    11 : \"Mauvaises expériences répétitives\" ,\n",
        "    #\n",
        "    12 : \"Peronnel désorganisé\" ,\n",
        "    #\n",
        "    13 : \"Suchis\" ,\n",
        "    #\n",
        "    14 : \"Endroit dangereux\" \n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "EtqzENyv7Li3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def predict(text,n_topics):\n",
        "  blob = TextBlob(text)\n",
        "  blob = blob.sentiment\n",
        "  value = blob[0]\n",
        "  if(value>0):\n",
        "    return value\n",
        "  elif(value<0):\n",
        "    print(\"Polarité : \", value,\" - COMMENTAIRE NEGATIF\")\n",
        "    text = preprocess_text(text)\n",
        "    text=[text]\n",
        "    X = vectorizer.transform(text)\n",
        "    topics_correlations = model.transform(X)\n",
        "    unsorted_topics_correlations=topics_correlations[0].copy()\n",
        "    topics_correlations[0].sort()\n",
        "    sorted=topics_correlations[0][::-1]\n",
        "    topics=[]\n",
        "    for i in range(n_topics):\n",
        "      corr_value= sorted[i]\n",
        "      result = np.where(unsorted_topics_correlations == corr_value)[0]\n",
        "      topics.append(labels.get(result[0]))\n",
        "    return value,topics\n",
        "  else:\n",
        "    return value"
      ],
      "metadata": {
        "id": "ZDvjl3Hc2eah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b=predict(p,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3Q4EILb576-",
        "outputId": "d85f4185-4f7f-4952-96ab-92d277aecce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarité :  -0.9099999999999998  - COMMENTAIRE NEGATIF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZ3SGTr159Qs",
        "outputId": "3f6cb3e6-442e-4ef5-8d08-6fd5581e7613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mauvaise expérience générale', 'Endroit dangereux', 'Peronnel désorganisé']"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pICKhoB6RdZ-",
        "outputId": "086ef97f-d1be-4565-eeb4-4c5963540835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.9099999999999998"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8-ZFPgpWYpc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}